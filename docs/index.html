<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-D2M3KMDV43"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-D2M3KMDV43');
</script>


<title>MDET Project Page</title>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<meta property="og:image" content="../images/ldet_teaser.gif"/>

<meta property="og:title" content="Learning to Detect Every Thing in an Open World"/>

<script src="lib.js" type="text/javascript"></script>
<script src="popup.js" type="text/javascript"></script>

<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';
</script>
<link media="all" href="glab.css" type="text/css" rel="StyleSheet">
<style type="text/css" media="all">

#primarycontent {
	MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
1000? "1000px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
1000px }
BODY {
	TEXT-ALIGN: center
}
</style>

<meta content="MSHTML 6.00.2800.1400" name="GENERATOR"><script src="b5m.js" id="b5mmain" type="text/javascript"></script>
</head>

<body>
<div id="primarycontent">
<center><h1>Mind the Backbone: Minimizing Backbone Distortion for Robust Object Detection </h1></center>
<center><h2>
	<a href="https://cs-people.bu.edu/keisaito/index.html">Kuniaki Saito</a><sup>1 </sup>&nbsp;&nbsp;&nbsp;
  <a href="https://cs-people.bu.edu/donhk/">Donghyun Kim</a><sup>2 </sup>&nbsp;&nbsp;&nbsp;
  <a href="https://cs-people.bu.edu/piotrt/">Piotr Teterwak</a><sup>1 </sup>&nbsp;&nbsp;&nbsp;
  <a href="https://www.rogerioferis.org/">Rogerio Feris</a><sup>2 </sup>&nbsp;&nbsp;&nbsp;
	<a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a><sup>1, 3</sup>&nbsp;&nbsp;&nbsp;
	</h2>

	<center><h2>
		<a> 1. Boston University </a> &nbsp;&nbsp;&nbsp;
		<a> 2. MIT-IBM Watson AI Lab </a> &nbsp;&nbsp;&nbsp;
		<a> 3. Meta </a> &nbsp;&nbsp;&nbsp;
	</h2></center>

<center><h2><strong><a href="https://arxiv.org/pdf/2303.14744.pdf">Paper</a> | <a href="https://github.com/VisionLearningGroup/mind_back">Code </a> </h2> </a></strong> </center>
<center>
<br>
<br>
<br>
<img src="images/teaser_github.png" width="70%"> </a></center>
<br>

<div style="font-size:14px" class="abstract">
  <h1 align="center">Abstract</h1>
  <p align="justify"> 
	Building object detectors that are robust to domain shifts
is critical for real-world applications. Prior approaches
fine-tune a pre-trained backbone and risk overfitting it to
in-distribution (ID) data and distorting features useful for
out-of-distribution (OOD) generalization. We propose to
use Relative Gradient Norm (RGN) as a way to measure the
vulnerability of a backbone to feature distortion, and show
that high RGN is indeed correlated with lower OOD performance. Our analysis of RGN yields interesting findings:
<b>some backbones lose OOD robustness during fine-tuning,
but others gain robustness because their architecture prevents the parameters from changing too much from the initial model. 
</b> Given these findings, we present recipes to boost
OOD robustness for both types of backbones. Specifically,
we investigate regularization and architectural choices for
minimizing gradient updates so as to prevent the tuned
backbone from losing generalizable features. Our proposed
techniques complement each other and show substantial
improvements over baselines on diverse architectures and
datasets. </p></div>


<a href="https://arxiv.org/pdf/2303.14744.pdf"><img style="float: left; padding: 10px; PADDING-RIGHT: 30px;" alt="paper thumbnail" src="images/thumbnail.png" width=170></a>



<h2>Paper</h2>
<p><a href="https://arxiv.org/pdf/2303.14744.pdf">arxiv</a>,  2023. </p>



<h2>Citation</h2>
<p>Kuniaki Saito, Donghyun Kim, Piotr Teterwak, Rogerio Feris, Kate Saenko<br>"Mind the Backbone: Minimizing Backbone Distortion for Robust Object Detection".
<a href="./bib.txt">Bibtex</a>

</p>


<h2><p><a href='https://github.com/VisionLearningGroup/mind_back'> Code </a>  </h2> </p>

<br>


<br>
<hr size="1px" width="100%" color="#999999">

<br>
<h1 align='center'> What is domain generalization?</h1>

<center><img src="images/dg_examples.png" width="700"/>
	<figcaption>Fig.1 - Left: training domains, Right: testing domains.</figcaption>
</center>
<br><br>

<p> Domain generalization refers to the ability of a machine learning model to generalize 
	its knowledge and perform well on new, unseen domains that differ from the domains it was trained on. 
	In traditional machine learning settings, models are trained and tested on data from the same domain, 
	assuming that the training and testing data follow the same distribution. 
	However, in real-world scenarios, the distribution of data can vary across different domains, 
	such as different geographical locations, different times, or different sources. 
	As shown in Fig.1, we aim to train a model generalizable on diverse domains given a single domain as training data. 
</p>

<hr size="1px" width="100%" color="#999999">

<h1 align='center'> Model distortion matters in domain generalization</h1>
	<center> <img src="images/fine-tuning.png" width="700" />
	<figcaption>Fig.2. Pre-trained model with generalizable representations can lose suffer from model distortion. </figcaption>
	</center>
<br>
	<p align="justify">
The task of domain generalization is actively explored in image classification. 
One effective way is to use a model pre-trained with large scale dataset and fine-tune it on the downstream task. 
The model has the knowledge about diverse categories and it learns invariance across different domains. 
But, if we naively fine-tune the model, the model may perform well on the domain of training data, but will not perform well on OOD. 
The backbone can lose generalizable representations by updating parameters too much to adapt to the training data. So, distorting model parameters can decrease performance on OOD.  
On the other hand, if we freeze the model parameters and only tune the linear head, model will not be distorted and may show high accuracy in OOD. But, it may not perform well near ID because the backbone is not adapted to the training data. 
An important message from work on image classification is that we need to care about the model distortion in fine-tuning a model. 
We analyze the behavior of backbone models in the object detection and propose simple techiques to train a model generalizable in diverse domains. 

	</p>

<hr size="1px" width="100%" color="#999999">

<h1 align='center'> Analysis on Model Distortion </h1>
	<center> <img src="images/table_distortion.png" width="700" /></center> 
	<figcaption>Fig.3. DP: Decoder probing, freezing pre-trained backbone. FT: Fine-tuning. </figcaption>
	<p align="justify">
		First, we start from comparing two approaches: freezing backbone (pre-trained model) on top of detector and plain fine-tuning. 
		Fig. 3 shows that network like ResNet50 shows significant decrease in OOD by updating backbone while other network gain performance by fine-tuning.
	</p>
<center> <img src="images/layer_wise.png" width="500" />
	<figcaption>Fig.4. Relative gradient norm (RGN) per layer for three models. Legend shows RGN averaged over all layers.</figcaption>	
</center>
<p align="justify">
	We attempt to quantify the model distortion by relative gradient norm, which is computing the norm of gradient over that of parameters. 
	Gradient is computed on detection loss. 
	We see that ResNet50 suffers from large model distortion while other two have relatively small RGN across many layers.
</p>
<hr size="1px" width="100%" color="#999999">

<h1 align='center'> Proposed Method </h1>
<p align="justify">
	Given the intuition from the analysis, we propose to regularize the backbone so that it can keep close to the initial parameters during fine-tuning. 
	We propose to apply both architecture- and objective-level regularization to achieve it. See our paper for more details. 
</p>
<hr size="1px" width="100%" color="#999999">

<h1 align='center'> Summary of Results </h1>
<center> <img src="images/results_summary.png" width="500" />
<figcaption>
Fig.5. Our proposed tuning methods show remarkable improvements in OOD for diverse backbones. 
</figcaption>
</center>

<p align="justify">
	We investigate the performance of our model on divese networks and datasets. 
	In summary, we find that the proposed fine-tuning is effective for diverse backbones as shown in Fig. 5. 
	Also, as in Fig. 6, it is effective for transformer-based backbone, image corruptions, and long-tailed object detection. 
</p>

<center> <img src="images/diverse_results.png" width="100%" /></center>

<figcaption>
	Fig.6. The proposed method is effective in Transformer-based backbone (Left), image corruptions (Center), and long-tailed object detection (Right). 
	</figcaption>
<hr size="1px" width="100%" color="#999999">
<h1 align='center'> Demo </h1>

<video width="700" controls>
	<source src="images/train_demo_concat.mp4" type="video/mp4">
  Your browser does not support the video tag.
  </video>
  <figcaption>
	Demo. Left: Baseline fine-tuning, Right: Our proposed fine-tuning. 
	</figcaption>
</body></html
>
